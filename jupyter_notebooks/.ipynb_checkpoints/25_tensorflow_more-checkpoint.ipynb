{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival -more TensorFlow.\n",
    "\n",
    "* Add droput layer after inputs\n",
    "* Separate out activation into separate layers\n",
    "* Separate out layer definition and flow\n",
    "* Use Leaky ReLu activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn for pre-processing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TensorFlow api model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data if not previously downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = False\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to calculate accuracy measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of accuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     1) observed positive rate: proportion of observed cases that are +ve\n",
    "     2) Predicted positive rate: proportion of predicted cases that are +ve\n",
    "     3) observed negative rate: proportion of observed cases that are -ve\n",
    "     4) Predicted negative rate: proportion of predicted cases that are -ve  \n",
    "     5) accuracy: proportion of predicted results that are correct    \n",
    "     6) precision: proportion of predicted +ve that are correct\n",
    "     7) recall: proportion of true +ve correctly identified\n",
    "     8) f1: harmonic mean of precision and recall\n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    11) positive likelihood: increased probability of true +ve if test +ve\n",
    "    12) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    13) false positive rate: proportion of false +ves in true -ve patients\n",
    "    14) false negative rate: proportion of false -ves in true +ve patients\n",
    "    15) true positive rate: Same as recall\n",
    "    16) true negative rate\n",
    "    17) positive predictive value: chance of true +ve if test +ve\n",
    "    18) negative predictive value: chance of true -ve if test -ve\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "    \n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    accuracy = np.mean(predicted == observed)\n",
    "    \n",
    "    precision = (np.sum(true_positives) /\n",
    "                 (np.sum(true_positives) + np.sum(false_positives)))\n",
    "        \n",
    "    recall = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    sensitivity = recall\n",
    "    \n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    \n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    \n",
    "    false_positive_rate = 1 - specificity\n",
    "    \n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    \n",
    "    true_positive_rate = sensitivity\n",
    "    \n",
    "    true_negative_rate = specificity\n",
    "    \n",
    "    positive_predictive_value = (np.sum(true_positives) / \n",
    "                                 np.sum(observed_positives))\n",
    "    \n",
    "    negative_predictive_value = (np.sum(true_negatives) / \n",
    "                                  np.sum(observed_positives))\n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['observed_positive_rate'] = np.mean(observed_positives)\n",
    "    results['observed_negative_rate'] = np.mean(observed_negatives)\n",
    "    results['predicted_positive_rate'] = np.mean(predicted_positives)\n",
    "    results['predicted_negative_rate'] = np.mean(predicted_negatives)\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1\n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['positive_likelihood'] = positive_likelihood\n",
    "    results['negative_likelihood'] = negative_likelihood\n",
    "    results['false_positive_rate'] = false_positive_rate\n",
    "    results['false_negative_rate'] = false_negative_rate\n",
    "    results['true_positive_rate'] = true_positive_rate\n",
    "    results['true_negative_rate'] = true_negative_rate\n",
    "    results['positive_predictive_value'] = positive_predictive_value\n",
    "    results['negative_predictive_value'] = negative_predictive_value\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to scale data\n",
    "\n",
    "In neural networks it is common to to scale input data 0-1 rather than use standardisation (subtracting mean and dividing by standard deviation) of each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "# Convert to NumPy as required for k-fold splits\n",
    "X_np = X.values\n",
    "y_np = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up neural net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, learning_rate=0.003):\n",
    "    \n",
    "    # Clear Tensorflow\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Define layers\n",
    "    input_layer = layers.Input(shape=number_features,  name='main_input')\n",
    "    dropout_1 = layers.Dropout(0.25)\n",
    "    dense_1 = layers.Dense(240)\n",
    "    activation_1 = layers.LeakyReLU(alpha=0.1)\n",
    "    norm_1 = layers.BatchNormalization()\n",
    "    dropout_2 = layers.Dropout(0.25)\n",
    "    dense_2 = layers.Dense(50)\n",
    "    activation_2 = layers.LeakyReLU(alpha=0.1)\n",
    "    dense_3 = layers.Dense(1)\n",
    "    output_layer = layers.Activation('sigmoid', name='sigmoid_output')\n",
    "    \n",
    "    # Define flow\n",
    "    inputs = input_layer\n",
    "    x = dropout_1(inputs)\n",
    "    x = dense_1(x)\n",
    "    x = activation_1(x)\n",
    "    x = norm_1(x)\n",
    "    x = dropout_2(x)\n",
    "    x = dense_2(x)\n",
    "    x = activation_2(x)\n",
    "    x = dense_3(x)\n",
    "    outputs = output_layer(x)\n",
    "    \n",
    "    # Construct net\n",
    "    net = Model(inputs, outputs)\n",
    "      \n",
    "    # Compiling model\n",
    "    net.compile(loss='binary_crossentropy',\n",
    "    optimizer=Adam(lr=learning_rate),\n",
    "    metrics=['accuracy'])\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show summary of the model structure\n",
    "\n",
    "Here we will create a model with 10 input features and show the structure of the model as  atable and as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 240)               2640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 240)               960       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                12050     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "sigmoid_output (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 15,701\n",
      "Trainable params: 15,221\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_net(10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# If necessary pip or conda install pydot and graphviz\n",
    "keras.utils.plot_model(model, \"titanic_tf_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dictionary of weights according to class distribution\n",
    "\n",
    "Here we will create a dictionary of class weights, in inverse proportion to how frequently a class occurs (to less frequent classes will be given more weight in model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proportion of each class\n",
    "prop_class_1 = y.mean()\n",
    "prop_class_0 = 1 - prop_class_1\n",
    "\n",
    "# Set weights in inverse propotion to occurance\n",
    "weight_class_0 = 1 / prop_class_0\n",
    "weight_class_1 = 1 / prop_class_1\n",
    "\n",
    "# Create dictionary of class weight\n",
    "class_weights = {0: weight_class_0, 1: weight_class_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model with k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_fold 1\n",
      "K_fold 2\n",
      "K_fold 3\n",
      "K_fold 4\n",
      "K_fold 5\n"
     ]
    }
   ],
   "source": [
    "# Set up lists to hold results\n",
    "training_acc_results = []\n",
    "test_acc_results = []\n",
    "\n",
    "# Set up splits\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "# Loop through the k-fold splits\n",
    "k_counter = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X_np, y_np):\n",
    "    k_counter +=1\n",
    "    print('K_fold {}'.format(k_counter))\n",
    "    \n",
    "    # Get X and Y train/test\n",
    "    X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "    y_train, y_test = y_np[train_index], y_np[test_index]\n",
    "    \n",
    "    # Scale X data\n",
    "    X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "    \n",
    "    # Define network\n",
    "    number_features = X_train_sc.shape[1]\n",
    "    model = make_net(number_features)\n",
    "    \n",
    "    # Define save checkpoint callback (only save if new best validation results)\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint('model_checkpoint.h5',\n",
    "                                                save_best_only=True)\n",
    "\n",
    "    # Define early stopping callback\n",
    "    # Stop when no validation improvement for 25 epochs)\n",
    "    # Restore weights to best validation accuracy\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience=25,\n",
    "                                                      restore_best_weights=True)\n",
    "\n",
    "    ### Train model\n",
    "    model.fit(X_train_sc,\n",
    "            y_train,\n",
    "            class_weight = class_weights,\n",
    "            epochs=250,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_test_sc, y_test),\n",
    "            verbose=0,\n",
    "            callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "            \n",
    "    ### Test model (print results for each k-fold iteration)\n",
    "    probability = model.predict(X_train_sc)\n",
    "    y_pred_train = probability >= 0.5\n",
    "    y_pred_train = y_pred_train.flatten()\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    training_acc_results.append(accuracy_train)\n",
    "\n",
    "    probability = model.predict(X_test_sc)\n",
    "    y_pred_test = probability >= 0.5\n",
    "    y_pred_test = y_pred_test.flatten()\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    test_acc_results.append(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show training and test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8103932584269663,\n",
       " 0.814866760168303,\n",
       " 0.8120617110799438,\n",
       " 0.820476858345021,\n",
       " 0.7980364656381487]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show individual accuracies on training data\n",
    "training_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7653631284916201,\n",
       " 0.8033707865168539,\n",
       " 0.7921348314606742,\n",
       " 0.7696629213483146,\n",
       " 0.8314606741573034]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show individual accuracies on test data\n",
    "test_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811, 0.792\n"
     ]
    }
   ],
   "source": [
    "# Get mean results\n",
    "mean_training = np.mean(training_acc_results)\n",
    "mean_test = np.mean(test_acc_results)\n",
    "\n",
    "# Display each to three decimal places\n",
    "print ('{0:.3f}, {1:.3}'.format(mean_training,mean_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results: Box Plot\n",
    "\n",
    "Box plots show median (orange line), the second and third quartiles (the box), the range (excluding outliers), and any outliers as 'whisker' points. Outliers, by convention, are conisiered to be any points outside of the quartiles +/- 1.5 times the interquartile range. The limit for outliers may be changed using the optional `whis` argument in the boxplot.\n",
    "\n",
    "Medians tend to be an easy reliable guide to the centre of a distribution (i.e. look at the medians to see whether a fit is improving or not, but also look at the box plot to see how much variability there is).\n",
    "\n",
    "Test sets tend to be more variable in their accuracy measures. Can you think why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEvCAYAAAATnJnNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVGElEQVR4nO3df5Bd5X3f8fcHAQUMyMLe0gBqRGcIIDNFSe4oNpliYuxYOMEMTT2gjOsp6ZiqgdrQlkL+6IDLtJ2YOJ6JYULohDgzZaC2wQm4acCl4MS4cbSyhUH8GG9EDDI0XiphGxwDEt/+ca/q6+UKXT3o6O5d3q+ZHd3znOec813t3c+e55x7zklVIUnadwdNugBJmlYGqCQ1MkAlqZEBKkmNDFBJamSASlKjgyddwP701re+tVatWjXpMiQtMZs2bXq2qmYWti+pAF21ahWzs7OTLkPSEpPkW6PaHcJLUiMDVJIaGaCS1MgAlaRGBqgkNTJAJamRASpJjQxQSWpkgEpSIwNUkhotqUs5paUsyaRL2C+W0mOEDFBpShyI4EmypAKua50O4ZOsS/J4krkkV42YvzzJXUkeTLIlyUWD9sOS/OVQ+8e6rFOSWnQWoEmWATcA5wCrgfVJVi/odgnwSFWdDpwFfCLJocCLwLsG7WuAdUne3lWtktSiyz3QtcBcVW2tqpeA24DzFvQp4Kj0D+4cCWwHdlbf84M+hwy+HFdIWlS6DNDjgaeGprcN2oZdD5wKPA08BHy0ql6B/h5sks3Ad4AvVtVXO6xVkvZZlwE66pThwr3I9wKbgePoD9WvT3I0QFXtqqo1wAnA2iSnjdxIcnGS2SSz8/Pz+696SdqLLgN0G7ByaPoE+nuawy4C7hgM2eeAJ4BThjtU1XPA/cC6URupqpuqqldVvZmZV91xX5I602WAbgROSnLi4MTQhcCdC/o8CZwNkORY4GRga5KZJG8etB8OvBt4rMNaJWmfdfY50KrameRS4G5gGXBzVW1JsmEw/0bgWuDTSR6iP+S/sqqeTfIPgT8cnMk/CPhMVX2hq1olqUWW0odme71e+VA5qZ0fpB8tyaaq6i1s91p4SWpkgEpSIwNUkhoZoJLUyACVpEYGqCQ1MkAlqZEBKkmNDFBJamSASlIjA1SSGhmgktTIAJWkRgaoJDUyQCWpkQEqSY0MUElqZIBKUiMDVJIaGaCS1MgAlaRGBqgkNTJAJamRASpJjQxQSWpkgEpSIwNUkhoZoJLUqNMATbIuyeNJ5pJcNWL+8iR3JXkwyZYkFw3aVya5L8mjg/aPdlmnJLXoLECTLANuAM4BVgPrk6xe0O0S4JGqOh04C/hEkkOBncC/qapTgbcDl4xYVpIm6uAO170WmKuqrQBJbgPOAx4Z6lPAUUkCHAlsB3ZW1TPAMwBV9f0kjwLHL1h2Seh/69OvqiZdgnTAdRmgxwNPDU1vA35uQZ/rgTuBp4GjgAuq6pXhDklWAT8NfLWrQiep6+BJYrhJHenyGOioXauFv8nvBTYDxwFrgOuTHP3/V5AcCdwOXFZV3xu5keTiJLNJZufn5/dP5ZI0hi4DdBuwcmj6BPp7msMuAu6ovjngCeAUgCSH0A/PW6rqjj1tpKpuqqpeVfVmZmb26zcgSa+lywDdCJyU5MTBiaEL6Q/Xhz0JnA2Q5FjgZGDr4Jjo7wOPVtVvd1ijJDXrLECraidwKXA38CjwmarakmRDkg2DbtcCZyR5CLgXuLKqngV+HvinwLuSbB58va+rWiWpRZcnkaiqPwH+ZEHbjUOvnwZ+ccRyX2b0MVRJWjS8EkmSGhmgktTIAJWkRgaoJDUyQCWpkQEqSY0MUElqZIBKUiMDVJIaGaCS1MgAlaRGBqgkNTJAJamRASpJjTq9nd1ScMwxx7Bjx45Jl/G6TPuD61asWMH27dsnXYb0KgboXuzYscOHsk3YtP8B0NLlEF6SGhmgktTIAJWkRgaoJDUyQCWpkQEqSY0MUElqZIBKUiMDVJIaeSXSXtTVR8M1yyddxhtaXX30pEuQRjJA9yIf+56Xck5YEuqaSVchvZpDeElqZIBKUqNOAzTJuiSPJ5lLctWI+cuT3JXkwSRbklw0NO/mJN9J8nCXNUpSq84CNMky4AbgHGA1sD7J6gXdLgEeqarTgbOATyQ5dDDv08C6ruqTpNery5NIa4G5qtoKkOQ24DzgkaE+BRyV/g0fjwS2AzsBqurPkqzqsL6xeT/KyVqxYsWkS5BG6jJAjweeGpreBvzcgj7XA3cCTwNHARdU1Ssd1rTPpv0MfJKp/x6kxarLY6CjdtsW/ia/F9gMHAesAa5Psk8f+ktycZLZJLPz8/NtlUpSgy4DdBuwcmj6BPp7msMuAu6ovjngCeCUfdlIVd1UVb2q6s3MzLyugiVpX3QZoBuBk5KcODgxdCH94fqwJ4GzAZIcC5wMbO2wJknabzoL0KraCVwK3A08CnymqrYk2ZBkw6DbtcAZSR4C7gWurKpnAZLcCvxv4OQk25L8865qlaQWWUonGHq9Xs3Ozk66jEXFk0jaF75fRkuyqap6C9u9EkmSGhmgktTIAJWkRgaoJDUyQCWpkQEqSY0MUElqZIBKUiMDVJIaGaCS1MgAlaRGBqgkNTJAJamRASpJjQxQSWpkgEpSIwNUkhrtNUCT/HISg1aSFhgnGC8Evpnk40lO7bogSZoWB++tQ1V9cPCs9vXAHyQp4A+AW6vq+10XKE2LY445hh07dky6jNctyaRLeF1WrFjB9u3bD8i29hqgAFX1vSS3A4cDlwHnA1ck+Z2q+lSXBUrTYseOHT6QbRE4kH8AxjkGem6SzwP/CzgEWFtV5wCnA/+24/okadEaZw/0A8Anq+rPhhur6gdJfq2bsiRp8RsnQK8Gntk9keRw4Niq+uuqurezyiRpkRvnLPxngVeGpncN2iTpDW2cAD24ql7aPTF4fWh3JUnSdBgnQOeTvH/3RJLzgGe7K0mSpsM4x0A3ALckuR4I8BTwoU6rkqQpsNc90Kr6q6p6O7AaWF1VZ1TV3DgrT7IuyeNJ5pJcNWL+8iR3JXkwyZYkF427rCRN2lgfpE/yS8DbgMN2f0i1qv7DXpZZBtwAvAfYBmxMcmdVPTLU7RLgkao6N8kM8HiSW+ifqNrbspI0UeN8kP5G4ALgX9Efwn8A+Mkx1r0WmKuqrYMTT7cB5y3oU8BR6afykcB2YOeYy0rSRI1zEumMqvoQsKOqPga8A1g5xnLH0z9eutu2Qduw64FTgaeBh4CPVtUrYy4rSRM1ToD+cPDvD5IcB7wMnDjGcqMuSF14ofB7gc3AccAa4PrBjUvGWba/keTiJLNJZufn58coS5L2j3EC9K4kbwauA74G/DVw6xjLbePH91RPoL+nOewi4I7qmwOeAE4Zc1kAquqmqupVVW9mZmaMsiRp/3jNk0iDGynfW1XPAbcn+QJwWFV9d4x1bwROSnIi8G369xX91QV9ngTOBv48ybHAycBW4LkxlpWkiXrNAK2qV5J8gv5xT6rqReDFcVZcVTuTXArcDSwDbq6qLUk2DObfCFwLfDrJQ/SH7VdW1bMAo5Zt+QYlqSvZ2/0Lk3wM+AaDofYBqapRr9er2dnZSZexqCTxHpUHiP/Xi0MXP4ckm6qqt7B9nM+B/mvgTcDOJD+kv6dYVXX0fq1QkqbMOI/0OOpAFCJJ02avAZrkzFHtC2+wrDYH4vEDB2IbDl31RjTOEP6KodeH0b9KaBPwrk4qeoMxeKTpNc4Q/tzh6SQrgY93VpEkTYlxPki/0DbgtP1diCRNm3GOgX6KH11GeRD9Sy4f7LIoSZoG4xwDHf5g5U7g1qp6oKN6JGlqjBOgnwN+WFW7oH+fzyRHVNUPui1Nkha3cY6B3gscPjR9OPA/uylHkqbHOAF6WFU9v3ti8PqI7kqSpOkwToC+kORndk8k+Vngb7srSZKmwzjHQC8DPptk9/04f4L+Iz4k6Q1tnA/Sb0xyCv17dQZ4rKpe7rwySVrkxnmo3CXAm6rq4ap6CDgyya93X5okLW7jHAP98OCO9ABU1Q7gw92VJEnTYZwAPShDt/MZPO/90O5KkqTpMM5JpLuBzwyeD1/ABuB/dFqVJE2BcQL0SuBi4F/SP4n0dfpn4iXpDW2vQ/iqegX4C/pPy+zRf4rmox3XJUmL3h73QJP8FP3HCa8H/i/w3wCq6hcOTGmStLi91hD+MeDPgXOrag4gyeUHpCpJmgKvNYT/FeD/APcl+S9JzqZ/DFSSxGsEaFV9vqouAE4B7gcuB45N8rtJfvEA1SdJi9Y4l3K+ANwC3JLkGOADwFXAPR3XJk2VuvpouGb5pMt4w6urjz5g28pSeipkr9er2dnZvXeUOpDEp6wuAl38HJJsqqrewvaWh8pJkjBAJamZASpJjToN0CTrkjyeZC7JVSPmX5Fk8+Dr4SS7BieqSPLRQduWJJd1WacktegsQAd3bboBOAdYDaxPsnq4T1VdV1VrqmoN8BvAl6pqe5LT6N8yby1wOvDLSU7qqlZJatHlHuhaYK6qtlbVS8BtwHmv0X89cOvg9anAX1TVD6pqJ/Al4PwOa5WkfdZlgB4PPDU0vW3Q9ipJjgDWAbcPmh4GzkzylsG89wEr97DsxUlmk8zOz8/vt+IlaW+6DNBRl33u6cNZ5wIPVNV2gKp6FPhN4IvAnwIPAjtHLVhVN1VVr6p6MzMzr79qSRpTlwG6jR/fazwBeHoPfS/kR8N3AKrq96vqZ6rqTGA78M1OqpSkRl0G6EbgpCQnJjmUfkjeubBTkuXAO4E/XtD+dwf//n3gH7MgYCVp0sa5I32TqtqZ5FL6jwRZBtxcVVuSbBjMv3HQ9XzgnsE198NuT/IW4GXgksHD7CRp0fBaeGk/8Vr4xcFr4SVpChigktTIAJWkRgaoJDUyQCWpkQEqSY0MUElqZIBKUiMDVJIaGaCS1MgAlaRGBqgkNTJAJamRASpJjQxQSWpkgEpSIwNUkhoZoJLUyACVpEYGqCQ1MkAlqZEBKkmNDFBJamSASlIjA1SSGhmgktTIAJWkRgaoJDXqNECTrEvyeJK5JFeNmH9Fks2Dr4eT7EpyzGDe5Um2DNpvTXJYl7VK0r7qLECTLANuAM4BVgPrk6we7lNV11XVmqpaA/wG8KWq2p7keOAjQK+qTgOWARd2VasktehyD3QtMFdVW6vqJeA24LzX6L8euHVo+mDg8CQHA0cAT3dWqSQ16DJAjweeGpreNmh7lSRHAOuA2wGq6tvAbwFPAs8A362qezqsVZL2WZcBmhFttYe+5wIPVNV2gCQr6O+tnggcB7wpyQdHbiS5OMlsktn5+fn9ULYkjafLAN0GrByaPoE9D8Mv5MeH7+8Gnqiq+ap6GbgDOGPUglV1U1X1qqo3MzOzH8qWpPF0GaAbgZOSnJjkUPoheefCTkmWA+8E/nio+Ung7UmOSBLgbODRDmuVpH12cFcrrqqdSS4F7qZ/Fv3mqtqSZMNg/o2DrucD91TVC0PLfjXJ54CvATuBrwM3dVWrJLVI1Z4OS06fXq9Xs7Ozky5Db1BJWEq/T9Oqi59Dkk1V1VvY7pVIktTIAJWkRgaoJDUyQCWpkQEqSY0MUElqZIBKUiMDVJIaGaCS1MgAlaRGBqgkNTJAJamRASpJjQxQSWpkgEpSIwNUkhoZoJLUyACVpEYGqCQ1MkAlqZEBKkmNDFBJamSASlIjA1SSGhmgktTIAJWkRgaoJDUyQCWpUacBmmRdkseTzCW5asT8K5JsHnw9nGRXkmOSnDzUvjnJ95Jc1mWtkrSvDu5qxUmWATcA7wG2ARuT3FlVj+zuU1XXAdcN+p8LXF5V24HtwJqh9Xwb+HxXtUpSiy73QNcCc1W1tapeAm4DznuN/uuBW0e0nw38VVV9q4MaJalZlwF6PPDU0PS2QdurJDkCWAfcPmL2hYwOVkmaqC4DNCPaag99zwUeGAzff7SC5FDg/cBn97iR5OIks0lm5+fnm4uVpH3V2TFQ+nucK4emTwCe3kPfPe1lngN8rar+Zk8bqaqbgJsAer3engJaOiCSUfsNOpBWrFhxwLbVZYBuBE5KciL9k0AXAr+6sFOS5cA7gQ+OWMeejotKi07V9P/9TrIkvo8DpbMAraqdSS4F7gaWATdX1ZYkGwbzbxx0PR+4p6peGF5+cFz0PcC/6KpGSXo9spT+2vR6vZqdnZ10GdLUcg90tCSbqqq3sN0rkSSpkQEqSY0MUElqZIBKUiMDVJIaGaCS1MgAlaRGBqgkNTJAJamRASpJjQxQSWpkgEpSoy5vZydpPzpQ9xrtejtL6WYlBqg0JZZS8CwVDuElqZEBKkmNDFBJamSASlIjA1SSGhmgktTIAJWkRgaoJDUyQCWpkQEqSY0MUElqlKV0fW2SeeBbk65jkXkr8Oyki9DU8P0y2k9W1czCxiUVoHq1JLNV1Zt0HZoOvl/2jUN4SWpkgEpSIwN06btp0gVoqvh+2QceA5WkRu6BSlIjH+mxyCR5C3DvYPLvAbuA+cH02qp66TWW7QEfqqqP7GUbX6mqM/ZHvZoOr+d9NVj+LOClqvpKZ0VOIYfwi1iSa4Dnq+q3htoOrqqdk6tK027U+6qLZd4IHMJPgSSfTvLbSe4DfjPJ2iRfSfL1wb8nD/qdleQLg9fXJLk5yf1Jtib5yND6nh/qf3+SzyV5LMktGTySMcn7Bm1fTvI7u9erpSPJzyb5UpJNSe5O8hOD9o8keSTJN5LclmQVsAG4PMnmJP9oknUvJg7hp8dPAe+uql1JjgbOrKqdSd4N/CfgV0YscwrwC8BRwONJfreqXl7Q56eBtwFPAw8AP59kFvi9wTaeSHJrR9+TJifAp4Dzqmo+yQXAfwR+DbgKOLGqXkzy5qp6LsmNuAf6Kgbo9PhsVe0avF4O/GGSk4ACDtnDMv+9ql4EXkzyHeBYYNuCPn9ZVdsAkmwGVgHPA1ur6olBn1uBi/fbd6LF4O8ApwFfHAw6lgHPDOZ9A7glyR8BfzSZ8qaDATo9Xhh6fS1wX1WdPxhe3b+HZV4cer2L0T/vUX3SXKWmRYAtVfWOEfN+CTgTeD/w75O87YBWNkU8BjqdlgPfHrz+Zx2s/zHgHwzCGeCCDrahyXoRmEnyDoAkhyR5W5KDgJVVdR/w74A3A0cC36d/KEhDDNDp9HHgPyd5gP7Qa7+qqr8Ffh340yRfBv4G+O7+3o4m6hXgn9A/KfkgsBk4g/776b8meQj4OvDJqnoOuAs435NIP86PMWmkJEdW1fODs/I3AN+sqk9Oui5pMXEPVHvy4cFJpS30Dxn83oTrkRYd90AlqZF7oJLUyACVpEYGqCQ1MkAlqZEBKkmNDFBJavT/AA3n/9nnhTRmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up X data \n",
    "x_for_box = [training_acc_results, test_acc_results]\n",
    "\n",
    "# Set up X labels\n",
    "labels = ['Training', 'Test'] \n",
    "\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "# Add subplot (can be used to define multiple plots in same figure)\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Define Box Plot (`widths` is optional)\n",
    "ax1.boxplot(x_for_box, \n",
    "            widths=0.7,\n",
    "            whis=100)\n",
    "\n",
    "# Set X and Y labels\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
